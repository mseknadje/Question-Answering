# -*- coding: utf-8 -*-
"""MS Question-Answering System Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ym4aqFRO23ZMpiWw5pMdv3Tn6A9aG8TM

### Question Answering: Project
This project aims to deploy an extractive Question Answering task to retrieve the answer to a question from a given text, by fine-tuning a Hugging Face Transformer model. The task is of extracting the answer to a question from a given context.

Paper: https://arxiv.org/pdf/1901.08634.pdf

Data: https://drive.google.com/drive/folders/1pW-Oqc8VmOokXSBV-U9Qnw8MlCV-C_t2?usp=sharing 

This project replicates the long and short answer dev and test performances in this research paper.
"""

!pip install datasets
!pip install transformers
# !pip install pytorch-transformers

"""# Preprocessing"""

from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, AutoModelForQuestionAnswering
from datasets import load_dataset

import torch
import numpy as np
import random

# we set up some seeds so that we can reproduce results
seed = 123
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

np.random.seed(seed)
random.seed(seed)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

"""
Some options for BERT model that can be run in colab:

"distilbert-base-uncased",
"distilbert-base-uncased-distilled-squad",
"distilbert-base-cased",
"distilbert-base-cased-distilled-squad",

"""

# dataset = load_dataset('cjlovering/natural-questions-short')
from datasets import load_dataset
from datasets import disable_caching

data_files = {"train": "train.json", "dev": "dev.json"}
dataset = load_dataset("cjlovering/natural-questions-short", data_files=data_files)

# disable_caching()
# dataset = load_dataset("cjlovering/natural-questions-short", keep_in_memory=True)

"""## 1. Loading the model and data"""

def load_model():
  """
  returns the BERT model and tokenizer
  """
  model_checkpoint = "distilbert-base-uncased"
  tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)
  model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
  return model, tokenizer

def load_data():
  """
  returns the training and validation datasets
  """
  train = dataset["train"]
  validation = dataset["dev"]
  return train, validation

"""## 2. Preprocessing and Tokenization"""

def tokenize(examples):
  """ helper method called in preprocess which converts the data into tokens,
  and maps the tokens to token start and end indices for each q-a pair
  """
  tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

  # list function that converts multiple dictionaries of inputs into a single dictionary
  flat = [item for sublist in examples["questions"] for item in sublist]
  bar = {
    k: [d.get(k) for d in flat]
    for k in set().union(*flat)
  }
  # converts the list of list into readable list of questions
  questions = list(bar.values())[0]
  contexts = examples["contexts"] # gets the contexts from dictionary
  answers = examples["answers"] # gets the answers from dictionary
  start_positions = []
  end_positions = []

  max_length = 512 # The maximum length of a feature we want to limit (question and context)
  inputs = tokenizer(questions, contexts, max_length=max_length, truncation="only_second", 
                     return_offsets_mapping=True, padding = "max_length")
  # getting the offsets allows us to later map string idx to token idx
  offsets = inputs.pop("offset_mapping")

  for i, offset in enumerate(offsets):
    answer = answers[i][0]
    start_char = answer["span_start"]
    end_char = answer["span_end"]
    sequence_ids = inputs.sequence_ids(i)

    # sequence_id = 0 for question, 1 for context, which tells us 
    # how to calculate the starting position of context and end.
    # finding index of context start
    idx = 0
    # finds the start index of context
    while sequence_ids[idx] != 1:
      # keep counting index until we reach the start of context
      idx += 1
    context_start = idx
    # finding the index of context end
    while sequence_ids[idx] == 1:
      # keeps counting index until we reach end of context
      idx += 1
    context_end = idx - 1

    # starts counting from context start to find token start index
    idx = context_start
    while idx <= context_end and offset[idx][0] <= start_char:
      idx += 1
    start_positions.append(idx - 1)
    # sets index to context end to count token end index
    idx = context_end
    while idx >= context_start and offset[idx][1] >= end_char:
      idx -= 1
    end_positions.append(idx + 1)

  
  inputs["start_positions"] = start_positions
  inputs["end_positions"] = end_positions

  return inputs

from transformers import default_data_collator

def preprocess_and_tokenize(data, batch_size):
  """ generic methods that maps inputted dataset into tokenized dataset using the method defined above
      generates and retursn the dataloader using the tokenized dataset and specifying the inputted batch size """
  # use map to apply tokenization preprocessing function to raw dataset in batches
  tokenized_dataset = data.map(tokenize, batched=True, remove_columns=data.column_names)
  # generate dataloader using tokenized dataset and specify batch size
  dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=batch_size, collate_fn=default_data_collator)
  return dataloader

"""## 3. Training

"""

from torch.optim import AdamW
from transformers import get_scheduler
from tqdm.auto import tqdm

def setup_train(model, train_dataloader, device):
  """ helper method called in training loop that sets up the parameters for training
  returns the optimizer, number of epochs, scheduler used in training loop
  """
  # we want to train on 5 epochs, batch size of 64, learning rate of 3e-4
  num_epochs = 5
  learning_rate = 0.0003
  num_update_steps_per_epoch = len(train_dataloader)
  # total number of training steps will be number of times needed to loop through
  # every batch, across all epochs so batch x epochs
  num_training_steps = num_epochs * num_update_steps_per_epoch

  optimizer = AdamW(model.parameters(), lr=learning_rate)
  # this loads all of the model parameters onto the device that we're using
  model.to(device)

  # instantiates scheduler object that takes in optimizer and params
  lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
  )

  return optimizer, num_epochs, lr_scheduler

import statistics

def train_loop(model, train_dataloader, validation_dataloader, device, batch_size):
  """ training loop that trains the inputted model using the training dataset
      also returns the calculated loss for both training and validation data"""
  # gets necessary parameters set up in helper method
  optimizer, num_epochs, lr_scheduler = setup_train(model, train_dataloader, device)
  # shows progress during training
  progress_bar = tqdm(range(len(train_dataloader)))

  # with 5 epochs, we make 5 complete passes through the training data
  for epoch in range(num_epochs):
    # puts model in training model
    model.train()
    print(f"Epoch {epoch + 1} training:")
    # initialize loss for epoch loss calculation
    train_loss = 0
    validation_loss = 0

    for step, batch in enumerate(train_dataloader):
      batchondevice = {}
      for key,value in batch.items():
        # applies batch to device
        batchondevice[key] = value.to(device)

      # obtains model outputs
      outputs = model(**batchondevice)
      # obtains and sums loss for training
      loss = outputs.loss
      train_loss += loss

      # updates the model and params
      model.zero_grad()
      loss.backward()
      optimizer.step()
      lr_scheduler.step()
      progress_bar.update(1)

    # sets model to eval mode since we set it to train for training
    model.eval()
    for step, batch in enumerate(validation_dataloader):
      valondevice = {}
      for key,value in batch.items():
        # applys batch inputs to device
        valondevice[key] = value.to(device)
      # removes iterative gradient since not necessary for validation
      with torch.no_grad():
        # gets model outputs and obtains loss
        val_outputs = model(**valondevice)
        val_loss = val_outputs.loss
        validation_loss += val_loss

    # loss is the average of all batches over the whole epoch
    # to obtain average, we can divde by total number of batches
    train_avg_loss = train_loss / len(train_dataloader)
    validation_avg_loss = validation_loss / len(validation_dataloader)

    # return train_losses, val_losses
    print(f"Train loss = {train_avg_loss}")
    print(f"Validation loss = {validation_avg_loss}")

  return train_avg_loss, validation_avg_loss

"""## 4. Evaluation"""

def compute_metrics(batch, start_label, end_label, pred_start, pred_end):
  """ helper method called in evaluation loop to calculate metrics for one example
      takes array of reference start, end labels, start and end predictions
      returns precision, recall, f1 scores for given example
  """
  for i in range(len(batch)):
    print(len(batch))
    # Let M = the number of matched tokens between the reference answer and the predicted answer by Bert. 
    matched_tokens = len(range(max(start_label[i], pred_start[i]), min(end_label[i], pred_end[i])+1))
    # Let N1= the number of tokens in the reference answer.
    num_ref_tokens = len(range(start_label[i], end_label[i]))
    # Let N2= the number of tokens in the predicted answer by Bert.
    num_pred_tokens = len(range(pred_start[i], pred_end[i]))

    # since we can't divide by 0, make precision = 0 whenever there are no predicted tokens
    if num_pred_tokens == 0:
      precision = 0
    elif num_pred_tokens != 0:
      precision = 1.0 * matched_tokens / num_pred_tokens
    
    # same as above
    if num_ref_tokens == 0:
      recall = 0
    elif num_ref_tokens != 0:
      recall = 1.0 * matched_tokens / num_ref_tokens
    
    # condition ensures we also aren't dividing by 0 for f1 scoring
    if precision == 0 and recall == 0:
      f1_score = 0
    elif precision != 0 or recall != 0:
    # Then precision = M / N2, recall = M / N1, F1 score = 2/(1/precision+1/recall).
      f1_score = (2 * precision * recall) / (precision + recall)
      
    return precision, recall, f1_score

def eval_loop(model, dataloader, device):
  """ evaluates the pretrained model using the dataloader containing the validation examples.
      returns evaluation metrics: precision, recall, f1
  """
  # sets model in eval mode
  model.eval()
  print("validation:")
  progress_bar = tqdm(range(len(dataloader)))

  # initialize arrays to store metrics for all examples
  total_recall = []
  total_precision = []
  total_f1 = []

  for batch in dataloader:
    # apply model on the batch inputs
    batchondevice = {}
    for key,value in batch.items():
      batchondevice[key] = value.to(device)
    # disable gradient calculation to speed up processing
    with torch.no_grad():
      outputs = model(**batchondevice)
    
    # gets the model outputs predictions
    start_logits = outputs.start_logits
    end_logits = outputs.end_logits
    
    # getting the predicted token indices (start and end)
    pred_start = start_logits.argmax(dim=-1).tolist()
    pred_end = end_logits.argmax(dim=-1).tolist()
    
    # getting the reference token indices
    start_label = batchondevice["start_positions"].tolist()
    end_label = batchondevice["end_positions"].tolist()

    # calls helper method to calculate precision, recall, f1 for each example
    precision, recall, f1_score = compute_metrics(batch, start_label, end_label, pred_start, pred_end)
    
    # store scores of individual examples to later computer average scores
    total_precision.append(precision)
    total_recall.append(recall)
    total_f1.append(f1_score)

  # gets the average metrics across all examples
  avg_precision = statistics.mean(total_precision)
  avg_recall = statistics.mean(total_recall)
  avg_f1 = statistics.mean(total_f1)

  return avg_precision, avg_recall, avg_f1

"""## Full run"""

def main():
  ''' basic structure of the main block to run program one step at a time'''
  device = "cuda" if torch.cuda.is_available() else "cpu"
  batch_size = 4

  model, tokenizer = load_model()
  train, validation = load_data()

  train_data_loader = preprocess_and_tokenize(train, batch_size)
  validation_data_loader = preprocess_and_tokenize(validation, batch_size)

  train_losses, val_losses = train_loop(model, train_data_loader, validation_data_loader, device, batch_size)
  precision, recall, f1_score  = eval_loop(model, validation_data_loader, device)

  
  print("PRECISION: ", precision)
  print("RECALL: ", recall)
  print("F1-SCORE: ", f1_score)

if __name__ == "__main__":
  main()
